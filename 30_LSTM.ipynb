{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model for Learning Purposes\n",
        "\n",
        "This notebook demonstrates the use of a **Long Short-Term Memory (LSTM)** model for a basic text processing task. The model is created for **learning purposes** and **educational demonstrations**, not for achieving high-level performance or production deployment.\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "- **Task**: The model is built to perform Next Word Predictor.\n",
        "- **Model**: The model architecture consists of an **Embedding** layer, followed by an **LSTM** layer, and a **Dense** output layer with a **softmax** activation.\n",
        "- **Dataset**: The dataset is small and may not be suitable for training a high-quality model. The model is more focused on demonstrating the basic structure and flow of working with LSTM networks.\n",
        "\n",
        "## Key Observations\n",
        "\n",
        "- **Overfitting**: As expected with small datasets, the model shows signs of **overfitting** where the training accuracy is much higher than the validation accuracy. The model performs well on the training set but struggles to generalize to the validation set.\n",
        "  \n",
        "- **Validation Accuracy**: The validation accuracy is low, and the validation loss is high, which is expected due to the small dataset and limited data diversity.\n",
        "\n",
        "## Important Notes\n",
        "\n",
        "- **Learning Focus**: This notebook is intended for beginners who are learning about LSTM networks and the Keras/TensorFlow API. The goal is to understand how to build, compile, and train an LSTM model for sequential data tasks.\n",
        "  \n",
        "- **Not for Production**: This model is **not optimized** for production use or real-world applications. The model architecture, hyperparameters, and training process can be adjusted and improved further when working with larger and more balanced datasets.\n",
        "\n",
        "- **Data Size**: The size of the data used is small, and more data would be needed to achieve better performance and generalization.\n",
        "\n",
        "- **Next Steps**: Once you are comfortable with the basics of LSTM networks, you can:\n",
        "  - Experiment with larger datasets.\n",
        "  - Try more advanced techniques such as **Bidirectional LSTM**, **attention mechanisms**, or **pre-trained embeddings**.\n",
        "  - Work on improving model regularization (e.g., adding **Dropout**, **L2 regularization**) to prevent overfitting.\n",
        "  - Explore hyperparameter tuning and model optimization techniques.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook serves as a simple introduction to working with LSTM models. It is important to remember that training a high-performing model requires careful data preprocessing, tuning, and experimenting with various architectures, and larger datasets. As a beginner, focus on understanding the basics of model architecture and training cycles before moving on to more complex tasks.\n",
        "\n",
        "---\n",
        "\n",
        "Happy learning and experimenting with LSTM models! 🚀\n"
      ],
      "metadata": {
        "id": "5QKAb0ZJPT5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0a8LWww-6eaW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text file\n",
        "with open(\"/content/Deep Learning Curriculum.txt\", \"r\") as file:\n",
        "    text = file.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0DFIvh6hFd"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ZMIlBKjq61Yv",
        "outputId": "8e643ab3-f442-4480-a00e-0fd139484bab"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CampusX Deep Learning Curriculum\\nA. Artificial Neural Network and how to improve them\\n1. Biological Inspiration\\n●\\n\\nUnderstanding the neuron structure\\n\\n●\\n\\nSynapses and signal transmission\\n\\n●\\n\\nHow biological concepts translate to artificial neurons\\n\\n2. History of Neural Networks\\n●\\n\\nEarly models (Perceptron)\\n\\n●\\n\\nBackpropagation and MLPs\\n\\n●\\n\\nThe \"AI Winter\" and resurgence of neural networks\\n\\n●\\n\\nEmergence of deep learning\\n\\n3. Perceptron and Multilayer Perceptrons (MLP)\\n●\\n\\nSingle-layer perceptron limitations\\n\\n●\\n\\nXOR problem and the need for hidden layers\\n\\n●\\n\\nMLP architecture\\n\\n4. Layers and Their Functions\\n●\\n\\nInput Layer\\n○\\n\\n●\\n\\nHidden Layers\\n○\\n\\n●\\n\\nAccepting input data\\nFeature extraction\\n\\nOutput Layer\\n○\\n\\nProducing final predictions\\n\\n5. Activation Functions\\n\\n\\x0c●\\n\\nSigmoid Function\\n○\\n\\n●\\n\\nHyperbolic Tangent (tanh)\\n○\\n\\n●\\n\\nAdvantages in mitigating vanishing gradients\\n\\nLeaky ReLU and Parametric ReLU\\n○\\n\\n●\\n\\nComparison with sigmoid\\n\\nReLU (Rectified Linear Unit)\\n○\\n\\n●\\n\\nCharacteristics and limitations\\n\\nAddressing the dying ReLU problem\\n\\nSoftmax Function\\n○\\n\\nMulti-class classification outputs\\n\\n6. Forward Propagation\\n●\\n\\nMathematical computations at each neuron\\n\\n●\\n\\nPassing inputs through the network to generate outputs\\n\\n7. Loss Functions\\n●\\n\\nMean Squared Error (MSE)\\n○\\n\\n●\\n\\nCross-Entropy Loss\\n○\\n\\n●\\n\\nUsed in classification tasks\\n\\nHinge Loss\\n○\\n\\n●\\n\\nUsed in regression tasks\\n\\nUsed with SVMs\\n\\nSelecting appropriate loss functions based on tasks\\n\\n8. Backpropagation\\n●\\n\\nDerivation using the chain rule\\n\\n●\\n\\nComputing gradients for each layer\\n\\n●\\n\\nUpdating weights and biases\\n\\n●\\n\\nUnderstanding computational graphs\\n\\n9. Gradient Descent Variants\\n●\\n\\nBatch Gradient Descent\\n○\\n\\nPros and cons\\n\\n\\x0c●\\n\\nStochastic Gradient Descent (SGD)\\n○\\n\\n●\\n\\nAdvantages in large datasets\\n\\nMini-Batch Gradient Descent\\n○\\n\\nBalancing between batch and SGD\\n\\n10. Optimization Algorithms\\n●\\n\\nMomentum\\n○\\n\\n●\\n\\nNesterov Accelerated Gradient\\n○\\n\\n●\\n\\nAdaptive learning rates\\n\\nRMSProp\\n○\\n\\n●\\n\\nLooking ahead to the future position\\n\\nAdaGrad\\n○\\n\\n●\\n\\nAccelerating SGD\\n\\nFixing AdaGrad\\'s diminishing learning rates\\n\\nAdam\\n○\\n\\nCombining momentum and RMSProp\\n\\n11. Regularization Techniques\\n●\\n\\nL1 and L2 Regularization\\n○\\n\\n●\\n\\nDropout\\n○\\n\\n●\\n\\nAdding penalty terms to the loss function\\nPreventing overfitting by randomly dropping neurons\\n\\nEarly Stopping\\n○\\n\\nHalting training when validation loss increases\\n\\n12. Hyperparameter Tuning\\n●\\n\\nLearning Rate\\n○\\n\\n●\\n\\nBatch Size\\n○\\n\\n●\\n\\nTrade-offs between speed and stability\\n\\nNumber of Epochs\\n○\\n\\n●\\n\\nImpact on convergence\\n\\nAvoiding overfitting\\n\\nNetwork Architecture\\n\\n\\x0c○\\n●\\n\\nDeciding depth and width\\n\\nTechniques:\\n○\\n\\nGrid search\\n\\n○\\n\\nRandom Search\\n\\n○\\n\\nBayesian optimization\\n\\n13. Vanishing and Exploding Gradients\\n●\\n\\nProblems in deep networks\\n\\n●\\n\\nSolutions:\\n○\\n\\nProper weight initialization\\n\\n○\\n\\nUse of ReLU activation functions\\n\\n14. Weight Initialization Strategies\\n●\\n\\nXavier/Glorot Initialization\\n\\n●\\n\\nHe Initialization\\n\\n15. Batch Normalization\\n●\\n\\nNormalizing inputs of each layer\\n\\n●\\n\\nAccelerating training\\n\\n●\\n\\nReducing dependence on initialization\\n\\nB. Convolution Neural Networks\\n1. Challenges with MLPs for Image Data\\n●\\n\\nHigh dimensionality\\n\\n●\\n\\nLack of spatial invariance\\n\\n2. Advantages of CNNs\\n●\\n\\nParameter sharing\\n\\n●\\n\\nLocal connectivity\\n\\n\\x0c3. Convolution Operation\\n●\\n\\n●\\n\\nUnderstanding Kernels/Filters\\n○\\n\\nEdge detection filters\\n\\n○\\n\\nFeature extraction\\n\\nMathematical Representation\\n○\\n\\n●\\n\\nHyperparameters\\n○\\n\\n●\\n\\nConvolution in 2D and 3D\\nKernel size, depth\\n\\nStride and Padding\\n○\\n\\nControlling output dimensions\\n\\n○\\n\\nTypes of padding: same vs. valid\\n\\n4. Activation Functions\\n●\\n\\nReLU (Rectified Linear Unit)\\n○\\n\\n●\\n\\nAdvantages over sigmoid/tanh\\n\\nVariants\\n○\\n\\nLeaky ReLU\\n\\n○\\n\\nELU (Exponential Linear Unit)\\n\\n5. Pooling Layers\\n●\\n\\n●\\n\\n●\\n\\nPurpose\\n○\\n\\nDimensionality reduction\\n\\n○\\n\\nTranslation invariance\\n\\nTypes of Pooling\\n○\\n\\nMax pooling\\n\\n○\\n\\nAverage pooling\\n\\nPooling Size and Stride\\n\\n6. Fully Connected Layers\\n●\\n\\nTransition from Convolutional Layers\\n\\n●\\n\\nFlattening\\n○\\n\\nConverting 2D features to 1D\\n\\n\\x0c7. Loss Functions\\n●\\n\\nCross-Entropy Loss for Classification\\n\\n●\\n\\nMean Squared Error for Regression\\n\\n8. CNN Architecture\\nLayer Stacking\\n●\\n\\nConvolutional -> Activation -> Pooling\\n\\nFeature Maps\\n●\\n\\nUnderstanding depth and channels\\n\\nVisualization\\n●\\n\\nInterpreting learned features\\n\\n9. Data Preprocessing Techniques - Data Normalization\\n●\\n\\nScaling Pixel Values\\n○\\n\\n0-1 normalization\\n\\n○\\n\\nStandardization (z-score)\\n\\n10. Data Preprocessing Techniques -Data Augmentation\\n●\\n\\n●\\n\\nTechniques\\n○\\n\\nRotation, flipping, cropping\\n\\n○\\n\\nColor jitter, noise addition\\n\\nPurpose\\n○\\n\\nReducing overfitting\\n\\n○\\n\\nIncreasing dataset diversity\\n\\nCNN Architectures and Innovations\\n11. LeNet-5\\n●\\n\\nArchitecture Details\\n\\n\\x0c○\\n●\\n\\nLayers, activations\\n\\nContributions\\n○\\n\\nHandwritten digit recognition\\n\\n12. AlexNet\\n●\\n\\n●\\n\\nBreakthroughs\\n○\\n\\nDeeper network\\n\\n○\\n\\nUse of ReLU\\n\\nImpact on ImageNet Challenge\\n\\n13. VGG Networks\\n●\\n\\nVGG-16 and VGG-19\\n\\n●\\n\\nDesign Philosophy\\n○\\n\\nUsing small filters (3x3)\\n\\n○\\n\\nDeep but uniform architecture\\n\\n14. Inception Networks (GoogLeNet)\\n●\\n\\nInception Modules\\n○\\n\\n●\\n\\nParallel convolutional layers\\n\\nMotivation\\n○\\n\\nEfficient computation\\n\\n15. ResNet (Residual Networks)\\n●\\n\\nResidual Blocks\\n○\\n\\nIdentity mappings\\n\\n○\\n\\nShortcut connections\\n\\n●\\n\\nSolving Vanishing Gradient Problem\\n\\n●\\n\\nVariants\\n○\\n\\nResNet-50, ResNet-101\\n\\n16. MobileNets\\n●\\n\\nDepthwise Separable Convolutions\\n\\n\\x0c●\\n\\nOptimizations for Mobile Devices\\n\\n17. Pre-trained Models & Transfer Learning\\n●\\n\\nUsing Models Trained on ImageNet\\n\\n●\\n\\nFine-Tuning vs. Feature Extraction\\n\\nObject Detection and Localization [Optional]\\n18. Traditional Methods\\n●\\n\\nSliding Window Approach\\n\\n19. Modern Architecture\\n●\\n\\nRegion-Based CNNs (R-CNN)\\n○\\n\\nR-CNN\\n\\n○\\n\\nFast R-CNN\\n\\n○\\n\\nFaster R-CNN\\n\\n●\\n\\nYou Only Look Once (YOLO)\\n\\n●\\n\\nSingle Shot MultiBox Detector (SSD)\\n\\n●\\n\\nMask R-CNN\\n○\\n\\nInstance segmentation\\n\\nSemantic Segmentation\\n20. Fully Convolutional Networks (FCN)\\n●\\n\\nReplacing Fully Connected Layers\\n\\n21. U-Net\\n●\\n\\nEncoder-Decoder Architecture\\n\\n●\\n\\nSkip Connections\\n\\nGenerative Models with CNNs\\n22. Autoencoders\\n\\n\\x0c●\\n\\nConvolutional Autoencoders\\n○\\n\\n●\\n\\nImage reconstruction\\n\\nVariational Autoencoders (VAE)\\n\\n23. Generative Adversarial Networks (GANs)\\n●\\n\\nDCGAN\\n○\\n\\n●\\n\\nUsing CNNs in GANs\\n\\nApplications\\n○\\n\\nImage generation\\n\\n○\\n\\nSuper-resolution\\n\\nC. Recurrent Neural Networks\\n\\n1. Architecture of RNNs\\n●\\n\\nSequential Data Challenges\\n\\n●\\n\\nBasic RNN Structure\\n\\n●\\n\\nMathematical Formulation\\n\\n●\\n\\nActivation Functions\\n\\n2. Forward Propagation Through Time\\n●\\n\\nSequence Input Processing\\n○\\n\\n●\\n\\nHandling variable-length sequences\\n\\nOutput Generation\\n○\\n\\nAt each time step or after the entire sequence\\n\\n3. Backpropagation Through Time (BPTT)\\n●\\n\\nUnfolding the RNN\\n○\\n\\n●\\n\\nCalculating Gradients\\n○\\n\\n●\\n\\nTreating RNN as a deep network over time\\nApplying the chain rule through time steps\\n\\nComputational Complexity\\n○\\n\\nMemory and computation considerations\\n\\n\\x0c4. Challenges in Training RNNs\\n●\\n\\nVanishing Gradients\\n○\\n\\n●\\n\\nExploding Gradients\\n○\\n\\n●\\n\\nGradients diminish over long sequences\\nGradients grow exponentially\\n\\nSolutions\\n○\\n\\nGradient clipping\\n\\n○\\n\\nAdvanced architectures (e.g., LSTMs, GRUs)\\n\\n5. LSTM\\n●\\n\\nLSTM core components\\n\\n●\\n\\nGates in LSTM\\n\\n●\\n\\nIntuition Behind LSTMs\\n\\n●\\n\\nBackpropagation Through Time\\n\\n6. GRU\\n○\\n\\nGRU core components\\n\\n○\\n\\nGates in GRU\\n\\n○\\n\\nIntuition Behind GRU\\n\\n○\\n\\nBackpropagation in GRUs\\n\\n○\\n\\nGRU vs LSTM\\n\\n6. Deep RNNs\\n○\\n\\nStacking RNN layers\\n\\n○\\n\\nVanishing and Exploding Gradients in Deep RNNs\\n\\n○\\n\\nUsing LSTM and GRU\\n\\n○\\n\\nSolution and techniques to overcome VGP and EGP\\n\\n○\\n\\nResidual Connections\\n\\n○\\n\\nRegularization\\n\\n7. Bidirectional RNNs\\n○\\n\\nMotivation behind Bidirectional RNNs\\n\\n\\x0c○\\n\\nBidirectional RNN architecture\\n\\n○\\n\\nForward and Backward pass\\n\\n○\\n\\nCombining outputs\\n\\n○\\n\\nBidirectional LSTM\\n\\n8. Applications of RNNs\\n○\\n\\nLanguage modeling - Next word prediction\\n\\n○\\n\\nSentiment Analysis\\n\\n○\\n\\nPOS Tagging\\n\\n○\\n\\nTime series forecasting\\n\\nSeq2Seq Networks\\n\\n1. Encoder-Decoder Networks\\nA. Introduction to Encoder-Decoder Architecture\\n●\\n\\nPurpose and Motivation\\n○\\n\\nHandling variable-length input and output sequences.\\n\\n○\\n\\nEssential for tasks like machine translation, text summarization, and\\nspeech recognition.\\n\\nB. Components of Encoder-Decoder Networks\\n●\\n\\nEncoder\\n○\\n\\nProcesses the input sequence and encodes it into a fixed-length\\ncontext vector.\\n\\n○\\n\\nArchitecture: Typically uses Recurrent Neural Networks (RNNs), Long\\nShort-Term Memory (LSTM), or Gated Recurrent Units (GRUs).\\n\\n\\x0c●\\n\\nDecoder\\n○\\n\\nGenerates the output sequence from the context vector.\\n\\n○\\n\\nArchitecture: Similar to the encoder but focuses on producing outputs.\\n\\nC. Mathematical Formulation\\n●\\n\\nEncoder and Decoder Equations\\n\\nD. Implementation Details\\n●\\n\\n●\\n\\nHandling Variable-Length Sequences\\n○\\n\\nPadding: Adding zeros to sequences to ensure uniform length.\\n\\n○\\n\\nMasking: Ignoring padded elements during computation.\\n\\nLoss Functions\\n○\\n\\nCross-Entropy Loss: Commonly used for classification tasks at each\\ntime step.\\n\\n●\\n\\nTraining Techniques\\n○\\n\\nTeacher Forcing: Using the actual output as the next input during\\ntraining to speed up convergence.\\n\\nE. Limitations of Basic Encoder-Decoder Models\\n●\\n\\nFixed-Length Context Vector Bottleneck\\n○\\n\\nDifficulty in capturing all necessary information from long input\\nsequences.\\n\\n●\\n\\nSolution Overview\\n○\\n\\nIntroduction of attention mechanisms to allow the model to focus on\\nrelevant parts of the input sequence.\\n\\n2. Attention Mechanisms and Their Types\\nA. Motivation for Attention\\n\\n\\x0c●\\n\\nOvercoming the Bottleneck\\n○\\n\\nAttention allows the model to access all encoder hidden states rather\\nthan compressing all information into a single context vector.\\n\\n●\\n\\nBenefits\\n○\\n\\nImproved performance on long sequences.\\n\\n○\\n\\nEnhanced ability to capture alignment between input and output\\nsequences.\\n\\nB. Types of Attention Mechanisms\\n1. Additive Attention (Bahdanau Attention)\\n●\\n\\nConcept\\n○\\n\\n●\\n\\nCalculates alignment scores using a feedforward network\\n\\nCharacteristics\\n○\\n\\nConsidered more computationally intensive due to additional\\nparameters.\\n\\n2. Multiplicative Attention (Luong Attention)\\n●\\n\\n●\\n\\nConcept\\n○\\n\\nCalculates alignment scores using dot products.\\n\\n○\\n\\nScaled Dot Product: Adjusts for dimensionality.\\n\\nCharacteristics\\n○\\n\\nMore efficient than additive attention.\\n\\nC. Attention Mechanism Steps\\n1.\\n\\nCalculate Alignment Scores\\n\\n2. Compute Attention Weights\\n3. Compute Context Vector\\n4. Update Decoder State\\n\\nD. Implementing Attention in Seq2Seq Models\\n●\\n\\nIntegration with Decoder\\n\\n\\x0c○\\n●\\n\\nModify the decoder to incorporate the context vector at each time step.\\n\\nTraining Adjustments\\n○\\n\\nBackpropagate through the attention mechanism.\\n\\nE. Visualization and Interpretation\\n●\\n\\nAttention Weights Matrix\\n○\\n\\nVisualizing which input tokens the model attends to during each output\\ngeneration step.\\n\\n●\\n\\nApplications\\n○\\n\\nError analysis.\\n\\n○\\n\\nModel interpretability.\\n\\n3. Transformer Architectures\\nA. Limitations of RNN-Based Seq2Seq Models\\n●\\n\\nSequential Processing\\n○\\n\\n●\\n\\nRNNs process inputs sequentially, hindering parallelization.\\n\\nLong-Term Dependencies\\n○\\n\\nDifficulty in capturing relationships between distant tokens.\\n\\nB. Introduction to Transformers\\n●\\n\\nKey Innovations\\n○\\n\\nSelf-Attention Mechanism: Allows the model to relate different\\npositions of a single sequence to compute representations.\\n\\n○\\n\\nPositional Encoding: Injects information about the position of the tokens\\nin the sequence.\\n\\n●\\n\\nAdvantages\\n○\\n\\nImproved parallelization.\\n\\n○\\n\\nBetter at capturing global dependencies.\\n\\n\\x0cC. Components of Transformer Architecture\\n1. Multi-Head Self-Attention\\n●\\n\\nConcept\\n○\\n\\n●\\n\\nMultiple attention mechanisms (heads) operating in parallel.\\n\\nProcess\\n○\\n\\nQuery (Q), Key (K), and Value (V) matrices are computed from input\\nembeddings.\\n\\n○\\n\\nThe attention mechanism calculates a weighted sum of the values, with\\nweights derived from the queries and keys.\\n\\n2. Positional Encoding\\n●\\n\\nPurpose\\n○\\n\\nSince transformers do not have recurrence or convolution, positional\\nencoding provides the model with information about the position of\\neach token.\\n\\n●\\n\\nTechniques\\n○\\n\\nSinusoidal Functions:\\n\\n○\\n\\nLearned Embeddings\\n\\n3. Feedforward Networks\\n●\\n\\nArchitecture\\n○\\n\\nPosition-wise fully connected layers applied independently to each\\nposition.\\n\\n●\\n\\nActivation Functions\\n○\\n\\nTypically ReLU or GELU.\\n\\n4. Layer Normalization\\n●\\n\\nPurpose\\n○\\n\\nNormalizes inputs across the features to stabilize and accelerate\\ntraining.\\n\\n5. Residual Connections\\n\\n\\x0c●\\n\\nPurpose\\n○\\n\\nHelps in training deeper networks by mitigating the vanishing gradient\\nproblem.\\n\\n●\\n\\nImplementation\\n○\\n\\nAdding the input of a layer to its output before applying the activation\\nfunction.\\n\\nD. Transformer Encoder-Decoder Structure\\n●\\n\\nEncoder Stack\\n○\\n\\n●\\n\\nComposed of multiple identical layers, each containing:\\n■\\n\\nMulti-head self-attention layer.\\n\\n■\\n\\nFeedforward network.\\n\\nDecoder Stack\\n○\\n\\nSimilar to the encoder but includes:\\n■\\n\\nMasked multi-head self-attention layer to prevent positions from\\nattending to subsequent positions.\\n\\n■\\n\\nEncoder-decoder attention layer.\\n\\nE. Implementing Transformers\\n●\\n\\nKey Steps\\n○\\n\\nEmbedding Layer: Converts input tokens into dense vectors.\\n\\n○\\n\\nAdding Positional Encoding: Combines positional information with\\nembeddings.\\n\\n○\\n\\nBuilding Encoder and Decoder Layers: Stack multiple layers as per the\\narchitecture.\\n\\n○\\n\\nOutput Layer: Generates final predictions, often followed by a softmax\\nfunction.\\n\\n4. Types of Transformers\\nA. BERT (Bidirectional Encoder Representations from Transformers)\\n\\n\\x0c●\\n\\nPurpose\\n○\\n\\nPre-training deep bidirectional representations by jointly conditioning\\non both left and right context.\\n\\n●\\n\\nArchitecture\\n○\\n\\n●\\n\\nUses only the encoder part of the transformer.\\n\\nPre-Training Objectives\\n○\\n\\nMasked Language Modeling (MLM): Predicting masked tokens in the\\ninput.\\n\\n○\\n\\nNext Sentence Prediction (NSP): Predicting if two sentences follow each\\nother.\\n\\nB. GPT (Generative Pre-trained Transformer)\\n●\\n\\nPurpose\\n○\\n\\n●\\n\\nFocused on language generation tasks.\\n\\nArchitecture\\n○\\n\\nUses only the decoder part of the transformer with masked\\nself-attention to prevent information flow from future tokens.\\n\\n●\\n\\nTraining Objective\\n○\\n\\nCausal Language Modeling (CLM): Predicting the next word in a\\nsequence.\\n\\nC. Other Notable Transformers\\n●\\n\\nRoBERTa\\n○\\n\\n●\\n\\nImproves on BERT by training with larger batches and more data.\\n\\nALBERT\\n○\\n\\nReduces model size by sharing parameters and factorizing\\nembeddings.\\n\\n●\\n\\nT5 (Text-to-Text Transfer Transformer)\\n○\\n\\nTreats every NLP task as a text-to-text problem.\\n\\n5. Fine-Tuning Transformers\\n\\n\\x0cA. Concept of Fine-Tuning\\n●\\n\\nTransfer Learning\\n○\\n\\nAdapting a pre-trained model to a downstream task with task-specific\\ndata.\\n\\nB. Steps in Fine-Tuning\\n1.\\n\\nLoading Pre-Trained Model\\n○\\n\\nUse pre-trained weights from models like BERT, GPT, etc.\\n\\n2. Modifying Output Layers\\n○\\n\\nReplace the final layer to suit the specific task (e.g., classification head).\\n\\n3. Adjusting Hyperparameters\\n○\\n\\nLearning rate, batch size, number of epochs.\\n\\n4. Training on Task-Specific Data\\n○\\n\\nUse labeled data relevant to the task.\\n\\nC. Best Practices\\n●\\n\\nLayer-Wise Learning Rates\\n○\\n\\n●\\n\\nApply different learning rates to different layers.\\n\\nAvoiding Catastrophic Forgetting\\n○\\n\\nUse smaller learning rates to prevent the model from losing pre-trained\\nknowledge.\\n\\n●\\n\\nRegularization Techniques\\n○\\n\\nDropout, weight decay.\\n\\nD. Common Fine-Tuning Tasks\\n●\\n\\nText Classification\\n\\n●\\n\\nNamed Entity Recognition\\n\\n●\\n\\nQuestion Answering\\n\\n●\\n\\nText Summarization\\n\\n\\x0c6. Pre-Training Transformers\\nA. Pre-Training Objectives\\n●\\n\\nMasked Language Modeling (MLM)\\n○\\n\\n●\\n\\nCausal Language Modeling (CLM)\\n○\\n\\n●\\n\\nPredicting masked tokens in the input sequence.\\nPredicting the next token given the previous tokens.\\n\\nSequence-to-Sequence Pre-Training\\n○\\n\\nUsed in models like T5.\\n\\nB. Data Preparation\\n●\\n\\nCorpus Selection\\n○\\n\\n●\\n\\nLarge and diverse datasets (e.g., Wikipedia, Common Crawl).\\n\\nTokenization Strategies\\n○\\n\\nWordPiece: Used by BERT.\\n\\n○\\n\\nByte-Pair Encoding (BPE): Used by GPT.\\n\\nC. Training Strategies\\n●\\n\\nDistributed Training\\n○\\n\\n●\\n\\nMixed Precision Training\\n○\\n\\n●\\n\\nUsing multiple GPUs or TPUs.\\nReduces memory usage and increases speed.\\n\\nOptimization Algorithms\\n○\\n\\nAdam optimizer with weight decay (AdamW).\\n\\nD. Challenges in Pre-Training\\n●\\n\\nCompute Resources\\n○\\n\\n●\\n\\nRequires significant computational power.\\n\\nData Quality\\n○\\n\\nNoisy data can affect model performance.\\n\\n\\x0cE. Evaluation of Pre-Trained Models\\n●\\n\\nBenchmarking\\n○\\n\\n●\\n\\nUsing datasets like GLUE, SQuAD to assess performance.\\n\\nAblation Studies\\n○\\n\\nUnderstanding the impact of different components.\\n\\n7. Optimizing Transformers\\nA. Computational Challenges\\n●\\n\\nHigh Memory Consumption\\n○\\n\\n●\\n\\nDue to self-attention mechanisms.\\n\\nLong Training Times\\n\\nB. Optimization Techniques\\n1. Efficient Attention Mechanisms\\n●\\n\\nSparse Attention\\n○\\n\\n●\\n\\nLinearized Attention (Linformer)\\n○\\n\\n●\\n\\nReduces the number of computations by focusing on local patterns.\\nApproximates attention to reduce complexity.\\n\\nReformer\\n○\\n\\nUses locality-sensitive hashing to reduce complexity.\\n\\n2. Model Compression\\n●\\n\\nQuantization\\n○\\n\\n●\\n\\nPruning\\n○\\n\\n●\\n\\nReducing the precision of weights (e.g., from 32-bit to 8-bit).\\nRemoving less important weights or neurons.\\n\\nKnowledge Distillation\\n\\n\\x0c○\\n\\nTraining a smaller model (student) to replicate the behavior of a larger\\nmodel (teacher).\\n\\nC. Hardware Considerations\\n●\\n\\nGPUs vs. TPUs\\n○\\n\\n●\\n\\nTPUs can offer faster computation for tensor operations.\\n\\nParallelism Strategies\\n○\\n\\nData Parallelism\\n■\\n\\n○\\n\\nDistributing data across multiple devices.\\n\\nModel Parallelism\\n■\\n\\nDistributing the model\\'s layers across devices.\\n\\nD. Software Tools\\n●\\n\\nOptimized Libraries\\n○\\n\\nHugging Face Transformers: Provides optimized implementations.\\n\\n○\\n\\nDeepSpeed: Optimizes memory and computation.\\n\\n○\\n\\nNVIDIA Apex: Enables mixed precision training.\\n\\n8. NLP Applications Using Transformers\\nA. Text Classification\\n●\\n\\nSentiment Analysis\\n○\\n\\n●\\n\\nClassifying text as positive, negative, or neutral.\\n\\nTopic Classification\\n○\\n\\nCategorizing text into predefined topics.\\n\\nB. Question Answering\\n●\\n\\nImplementing QA Systems\\n○\\n\\nUsing models like BERT to find answers within a context.\\n\\n\\x0c●\\n\\nDatasets\\n○\\n\\nSQuAD, TriviaQA.\\n\\nC. Machine Translation\\n●\\n\\nTransformer Models\\n○\\n\\n●\\n\\nImplementing translation systems without RNNs.\\n\\nDatasets\\n○\\n\\nWMT datasets.\\n\\nD. Text Summarization\\n●\\n\\nAbstractive Summarization\\n○\\n\\n●\\n\\nGenerating concise summaries using models like T5.\\n\\nDatasets\\n○\\n\\nCNN/Daily Mail, Gigaword.\\n\\nE. Language Generation\\n●\\n\\nChatbots\\n○\\n\\n●\\n\\nCreating conversational agents using GPT models.\\n\\nStory Generation\\n○\\n\\nGenerating coherent narratives.\\n\\nF. Named Entity Recognition\\n●\\n\\nSequence Labeling\\n○\\n\\n●\\n\\nIdentifying entities like names, locations, dates.\\n\\nFine-Tuning\\n○\\n\\nAdapting pre-trained models for NER tasks.\\n\\n\\x0c\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = re.sub(r'[\\u2022\\u25CB●]\\s*', '', text)\n",
        "\n",
        "# Optional: Remove extra newlines and spaces that might result from the bullet removal\n",
        "# text = re.sub(r'\\n+', '\\n', text)  # Replace multiple newlines with a single newline\n",
        "# text = text.strip()  # Remove leading and trailing whitespaces\n"
      ],
      "metadata": {
        "id": "NCNFxEVV6EHi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faqs = \"\"\"About the Program\n",
        "What is the course fee for  Data Science Mentorship Program (DSMP 2023)\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Python Fundamentals\n",
        "Python libraries for Data Science\n",
        "Data Analysis\n",
        "SQL for Data Science\n",
        "Maths for Machine Learning\n",
        "ML Algorithms\n",
        "Practical ML\n",
        "MLOPs\n",
        "Case studies\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390\n",
        "Will Deep Learning and NLP be a part of this program?\n",
        "No, NLP and Deep Learning both are not a part of this program’s curriculum.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.\n",
        "Where can I find the class schedule?\n",
        "Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "What is the time duration of all the live sessions?\n",
        "Roughly, all the sessions last 2 hours.\n",
        "What is the language spoken by the instructor during the sessions?\n",
        "Hinglish\n",
        "How will I be informed about the upcoming class?\n",
        "You will get a mail from our side before every paid session once you become a paid user.\n",
        "Can I do this course if I am from a non-tech background?\n",
        "Yes, absolutely.\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "If I join/pay in the middle, will I be able to see all the past lectures?\n",
        "Yes, once you make the payment you will be able to see all the past content in your dashboard.\n",
        "Where do I have to submit the task?\n",
        "You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.\n",
        "Will we do case studies in the program?\n",
        "Yes.\n",
        "Where can we contact you?\n",
        "You can mail us at nitish.campusx@gmail.com\n",
        "Payment/Registration related questions\n",
        "Where do we have to make our payments? Your YouTube channel or website?\n",
        "You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/\n",
        "Can we pay the entire amount of Rs 5600 all at once?\n",
        "Unfortunately no, the program follows a monthly subscription model.\n",
        "What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb\n",
        "15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.\n",
        "What if I don’t like the course after making the payment. What is the refund policy?\n",
        "You get a 7 days refund period from the day you have made the payment.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmail.com\n",
        "Post registration queries\n",
        "Till when can I view the paid videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.\n",
        "But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.\n",
        "Why lifetime validity is not provided?\n",
        "Because of the low course fee.\n",
        "Where can I reach out in case of a doubt after the session?\n",
        "You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session\n",
        "If I join the program late, can I still ask past week doubts?\n",
        "Yes, just select past week doubt in the doubt clearance google form.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmai.com\n",
        "Certificate and Placement Assistance related queries\n",
        "What is the criteria to get the certificate?\n",
        "There are 2 criterias:\n",
        "You have to pay the entire fee of Rs 5600\n",
        "You have to attempt all the course assessments.\n",
        "I am joining late. How can I pay payment of the earlier months?\n",
        "You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.\n",
        "I have read that Placement assistance is a part of this program. What comes under Placement assistance?\n",
        "This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance\n",
        "Portfolio Building sessions\n",
        "Soft skill sessions\n",
        "Sessions with industry mentors\n",
        "Discussion on Job hunting strategies\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MzPB9hImJ8SX"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "SR-5-YmK6VlH"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "ekujL1Zp-WLn"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "BNGkYnva-s0U"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tJdLnPE-voo",
        "outputId": "932dcda4-b891-45b6-b8fb-98562b19dbbe"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "839"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for sentences in text.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentences])[0]\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "J2xBAbqQ_vUF"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max([len(x) for x in input_sequences])"
      ],
      "metadata": {
        "id": "rFHqG5zd_aic"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ilVmn9zGXqm",
        "outputId": "609208b8-4c30-460e-b47f-af13406fa492"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen, padding='pre')"
      ],
      "metadata": {
        "id": "GehRNW7BA_Y3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpCLG01DCXSR",
        "outputId": "6f2ba765-b9c5-4fc5-984d-19f3139e23b2"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0, 368,  46],\n",
              "       [  0,   0,   0, ..., 368,  46,  27],\n",
              "       [  0,   0,   0, ...,  46,  27, 369],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  44,  16,  24],\n",
              "       [  0,   0,   0, ...,  16,  24, 838],\n",
              "       [  0,   0,   0, ...,  24, 838,  48]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_sequences[:,:-1]\n",
        "y = padded_sequences[:,-1]"
      ],
      "metadata": {
        "id": "3BsXLwjxCbdw"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MhFw0nwCvOw",
        "outputId": "bcb51f6a-cd3c-4e32-a239-22880e784101"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1686, 12)\n",
            "(1686,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "d2qZE3fECzBf"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "\n"
      ],
      "metadata": {
        "id": "qMa7czYyC_Ax"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# model.add(Embedding(input_dim=(len(tokenizer.word_index)+1,), output_dim=100))\n",
        "model.add(Embedding(len(tokenizer.word_index)+1, 100, input_shape=(maxlen,)))\n",
        "model.add(Dropout(0.5))  # Dropout layer\n",
        "model.add(Bidirectional(LSTM(200))) # Added return_sequences=True\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnkdIEOMDNgT",
        "outputId": "d3b3321c-26f5-421c-f20a-5b596618e5da"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9-c4ZH_rEgAk"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "p5IEra1ZFJGy",
        "outputId": "a63f46c9-d5a7-4d9d-aa39-02185adeddbd"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │          \u001b[38;5;34m84,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │         \u001b[38;5;34m481,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m840\u001b[0m)                 │         \u001b[38;5;34m336,840\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">84,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">481,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">840</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">336,840</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m902,440\u001b[0m (3.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">902,440</span> (3.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m902,440\u001b[0m (3.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">902,440</span> (3.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)"
      ],
      "metadata": {
        "id": "x4SzDho-HBUo"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X,y, epochs=100, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEkP5IuNFJ3-",
        "outputId": "6572bfec-5e4f-42c9-e008-5eb228537d26"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.0166 - loss: 6.6166 - val_accuracy: 0.0118 - val_loss: 6.5587\n",
            "Epoch 2/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0313 - loss: 5.8853 - val_accuracy: 0.0296 - val_loss: 6.6933\n",
            "Epoch 3/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0356 - loss: 5.7959 - val_accuracy: 0.0178 - val_loss: 6.9121\n",
            "Epoch 4/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0447 - loss: 5.7753 - val_accuracy: 0.0296 - val_loss: 6.9027\n",
            "Epoch 5/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0364 - loss: 5.7336 - val_accuracy: 0.0296 - val_loss: 7.0153\n",
            "Epoch 6/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0319 - loss: 5.6451 - val_accuracy: 0.0089 - val_loss: 7.4012\n",
            "Epoch 7/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0370 - loss: 5.5770 - val_accuracy: 0.0148 - val_loss: 7.3293\n",
            "Epoch 8/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0423 - loss: 5.4686 - val_accuracy: 0.0089 - val_loss: 7.5914\n",
            "Epoch 9/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0473 - loss: 5.4077 - val_accuracy: 0.0148 - val_loss: 7.2461\n",
            "Epoch 10/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0395 - loss: 5.3531 - val_accuracy: 0.0118 - val_loss: 7.3974\n",
            "Epoch 11/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0499 - loss: 5.1994 - val_accuracy: 0.0148 - val_loss: 7.6745\n",
            "Epoch 12/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0574 - loss: 5.1095 - val_accuracy: 0.0148 - val_loss: 7.7838\n",
            "Epoch 13/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0473 - loss: 5.0432 - val_accuracy: 0.0089 - val_loss: 7.5776\n",
            "Epoch 14/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0588 - loss: 4.9526 - val_accuracy: 0.0118 - val_loss: 7.8311\n",
            "Epoch 15/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0846 - loss: 4.7438 - val_accuracy: 0.0118 - val_loss: 7.9130\n",
            "Epoch 16/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0767 - loss: 4.7206 - val_accuracy: 0.0118 - val_loss: 8.1192\n",
            "Epoch 17/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0746 - loss: 4.6727 - val_accuracy: 0.0089 - val_loss: 8.2096\n",
            "Epoch 18/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0885 - loss: 4.5853 - val_accuracy: 0.0089 - val_loss: 8.4196\n",
            "Epoch 19/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0882 - loss: 4.4504 - val_accuracy: 0.0118 - val_loss: 8.2572\n",
            "Epoch 20/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0922 - loss: 4.3879 - val_accuracy: 0.0030 - val_loss: 8.6483\n",
            "Epoch 21/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0932 - loss: 4.3014 - val_accuracy: 0.0059 - val_loss: 8.5975\n",
            "Epoch 22/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.1196 - loss: 4.1592 - val_accuracy: 0.0237 - val_loss: 8.6644\n",
            "Epoch 23/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1231 - loss: 4.0628 - val_accuracy: 0.0296 - val_loss: 8.7151\n",
            "Epoch 24/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1188 - loss: 3.9881 - val_accuracy: 0.0296 - val_loss: 8.8427\n",
            "Epoch 25/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1539 - loss: 3.7607 - val_accuracy: 0.0414 - val_loss: 9.0387\n",
            "Epoch 26/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1540 - loss: 3.6937 - val_accuracy: 0.0325 - val_loss: 9.2038\n",
            "Epoch 27/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1947 - loss: 3.5360 - val_accuracy: 0.0296 - val_loss: 9.0618\n",
            "Epoch 28/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2120 - loss: 3.3964 - val_accuracy: 0.0325 - val_loss: 9.1358\n",
            "Epoch 29/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2301 - loss: 3.3914 - val_accuracy: 0.0414 - val_loss: 9.2949\n",
            "Epoch 30/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2518 - loss: 3.1796 - val_accuracy: 0.0473 - val_loss: 9.2351\n",
            "Epoch 31/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2826 - loss: 3.1082 - val_accuracy: 0.0533 - val_loss: 9.4903\n",
            "Epoch 32/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3015 - loss: 2.9631 - val_accuracy: 0.0414 - val_loss: 9.5609\n",
            "Epoch 33/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3075 - loss: 2.8466 - val_accuracy: 0.0355 - val_loss: 9.4986\n",
            "Epoch 34/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3120 - loss: 2.8493 - val_accuracy: 0.0473 - val_loss: 9.7333\n",
            "Epoch 35/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3570 - loss: 2.6794 - val_accuracy: 0.0503 - val_loss: 9.6562\n",
            "Epoch 36/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3809 - loss: 2.5400 - val_accuracy: 0.0562 - val_loss: 9.8006\n",
            "Epoch 37/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3901 - loss: 2.5555 - val_accuracy: 0.0473 - val_loss: 9.9693\n",
            "Epoch 38/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4229 - loss: 2.4140 - val_accuracy: 0.0592 - val_loss: 10.0377\n",
            "Epoch 39/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4354 - loss: 2.2758 - val_accuracy: 0.0562 - val_loss: 10.1074\n",
            "Epoch 40/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4427 - loss: 2.3062 - val_accuracy: 0.0592 - val_loss: 9.9709\n",
            "Epoch 41/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4804 - loss: 2.1434 - val_accuracy: 0.0533 - val_loss: 10.0878\n",
            "Epoch 42/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4980 - loss: 2.1008 - val_accuracy: 0.0592 - val_loss: 10.1195\n",
            "Epoch 43/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5169 - loss: 2.0558 - val_accuracy: 0.0562 - val_loss: 10.2411\n",
            "Epoch 44/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5134 - loss: 1.9543 - val_accuracy: 0.0503 - val_loss: 10.1759\n",
            "Epoch 45/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5198 - loss: 1.8580 - val_accuracy: 0.0562 - val_loss: 10.2513\n",
            "Epoch 46/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5497 - loss: 1.7820 - val_accuracy: 0.0533 - val_loss: 10.2084\n",
            "Epoch 47/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5529 - loss: 1.7715 - val_accuracy: 0.0533 - val_loss: 10.5066\n",
            "Epoch 48/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5763 - loss: 1.5806 - val_accuracy: 0.0592 - val_loss: 10.5340\n",
            "Epoch 49/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5832 - loss: 1.6672 - val_accuracy: 0.0651 - val_loss: 10.5468\n",
            "Epoch 50/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6093 - loss: 1.5275 - val_accuracy: 0.0533 - val_loss: 10.4478\n",
            "Epoch 51/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6302 - loss: 1.4731 - val_accuracy: 0.0651 - val_loss: 10.4837\n",
            "Epoch 52/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6025 - loss: 1.5320 - val_accuracy: 0.0621 - val_loss: 10.7126\n",
            "Epoch 53/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6419 - loss: 1.4190 - val_accuracy: 0.0592 - val_loss: 10.6148\n",
            "Epoch 54/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6366 - loss: 1.3563 - val_accuracy: 0.0592 - val_loss: 10.6873\n",
            "Epoch 55/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6646 - loss: 1.3689 - val_accuracy: 0.0621 - val_loss: 10.6386\n",
            "Epoch 56/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6595 - loss: 1.3042 - val_accuracy: 0.0592 - val_loss: 10.7638\n",
            "Epoch 57/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6563 - loss: 1.2482 - val_accuracy: 0.0592 - val_loss: 10.6929\n",
            "Epoch 58/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6687 - loss: 1.2614 - val_accuracy: 0.0592 - val_loss: 10.7280\n",
            "Epoch 59/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6765 - loss: 1.2376 - val_accuracy: 0.0592 - val_loss: 10.7168\n",
            "Epoch 60/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7098 - loss: 1.1469 - val_accuracy: 0.0651 - val_loss: 11.0588\n",
            "Epoch 61/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6956 - loss: 1.1932 - val_accuracy: 0.0710 - val_loss: 10.5696\n",
            "Epoch 62/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7146 - loss: 1.1054 - val_accuracy: 0.0562 - val_loss: 10.8986\n",
            "Epoch 63/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7107 - loss: 1.0799 - val_accuracy: 0.0621 - val_loss: 10.9699\n",
            "Epoch 64/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7431 - loss: 1.0179 - val_accuracy: 0.0621 - val_loss: 10.8513\n",
            "Epoch 65/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7188 - loss: 1.0111 - val_accuracy: 0.0621 - val_loss: 10.9723\n",
            "Epoch 66/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7622 - loss: 0.9666 - val_accuracy: 0.0651 - val_loss: 11.0395\n",
            "Epoch 67/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7187 - loss: 1.0601 - val_accuracy: 0.0592 - val_loss: 10.8863\n",
            "Epoch 68/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7493 - loss: 0.9295 - val_accuracy: 0.0562 - val_loss: 10.8828\n",
            "Epoch 69/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7683 - loss: 0.9064 - val_accuracy: 0.0621 - val_loss: 11.0274\n",
            "Epoch 70/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7653 - loss: 0.9023 - val_accuracy: 0.0651 - val_loss: 10.8480\n",
            "Epoch 71/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7567 - loss: 0.9200 - val_accuracy: 0.0710 - val_loss: 11.0379\n",
            "Epoch 72/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7604 - loss: 0.9147 - val_accuracy: 0.0562 - val_loss: 10.9606\n",
            "Epoch 73/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7642 - loss: 0.8945 - val_accuracy: 0.0562 - val_loss: 10.9572\n",
            "Epoch 74/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7799 - loss: 0.8867 - val_accuracy: 0.0651 - val_loss: 11.1895\n",
            "Epoch 75/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7934 - loss: 0.7878 - val_accuracy: 0.0592 - val_loss: 11.0412\n",
            "Epoch 76/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7927 - loss: 0.7916 - val_accuracy: 0.0592 - val_loss: 11.1213\n",
            "Epoch 77/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7804 - loss: 0.7997 - val_accuracy: 0.0592 - val_loss: 11.1449\n",
            "Epoch 78/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7776 - loss: 0.8015 - val_accuracy: 0.0533 - val_loss: 11.3024\n",
            "Epoch 79/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8112 - loss: 0.7668 - val_accuracy: 0.0562 - val_loss: 10.9974\n",
            "Epoch 80/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7814 - loss: 0.7783 - val_accuracy: 0.0562 - val_loss: 11.3237\n",
            "Epoch 81/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8190 - loss: 0.7203 - val_accuracy: 0.0621 - val_loss: 11.0920\n",
            "Epoch 82/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7959 - loss: 0.7369 - val_accuracy: 0.0621 - val_loss: 11.1873\n",
            "Epoch 83/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7958 - loss: 0.7199 - val_accuracy: 0.0562 - val_loss: 11.4168\n",
            "Epoch 84/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8120 - loss: 0.6649 - val_accuracy: 0.0621 - val_loss: 11.2851\n",
            "Epoch 85/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7948 - loss: 0.6897 - val_accuracy: 0.0621 - val_loss: 11.4061\n",
            "Epoch 86/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8403 - loss: 0.6267 - val_accuracy: 0.0621 - val_loss: 11.1873\n",
            "Epoch 87/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8339 - loss: 0.6362 - val_accuracy: 0.0680 - val_loss: 11.2917\n",
            "Epoch 88/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8148 - loss: 0.6765 - val_accuracy: 0.0621 - val_loss: 11.2259\n",
            "Epoch 89/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8251 - loss: 0.6087 - val_accuracy: 0.0651 - val_loss: 11.4966\n",
            "Epoch 90/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8291 - loss: 0.6107 - val_accuracy: 0.0651 - val_loss: 11.4257\n",
            "Epoch 91/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8502 - loss: 0.5699 - val_accuracy: 0.0621 - val_loss: 11.5479\n",
            "Epoch 92/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8405 - loss: 0.6274 - val_accuracy: 0.0651 - val_loss: 11.4979\n",
            "Epoch 93/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8516 - loss: 0.5377 - val_accuracy: 0.0621 - val_loss: 11.3125\n",
            "Epoch 94/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8290 - loss: 0.6283 - val_accuracy: 0.0680 - val_loss: 11.3313\n",
            "Epoch 95/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8344 - loss: 0.5903 - val_accuracy: 0.0680 - val_loss: 11.5303\n",
            "Epoch 96/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8266 - loss: 0.5657 - val_accuracy: 0.0680 - val_loss: 11.3953\n",
            "Epoch 97/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8372 - loss: 0.5969 - val_accuracy: 0.0680 - val_loss: 11.4469\n",
            "Epoch 98/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8331 - loss: 0.5794 - val_accuracy: 0.0651 - val_loss: 11.5470\n",
            "Epoch 99/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8295 - loss: 0.5711 - val_accuracy: 0.0592 - val_loss: 11.3605\n",
            "Epoch 100/100\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8459 - loss: 0.5230 - val_accuracy: 0.0680 - val_loss: 11.6031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uLb_SWaHHIT6"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "g2pDDxDUKbSE",
        "outputId": "2b49cc9d-217d-4e07-d407-3383ae61eb40"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7961045993d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 148
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "text = \"lstm\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fDgJTw-KqiU",
        "outputId": "a578fcc3-0e0a-4696-c543-b0389942ae18"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "lstm analysis\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "lstm analysis or\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "lstm analysis or networks\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "lstm analysis or networks techniques\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "lstm analysis or networks techniques to\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "lstm analysis or networks techniques to overcome\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "lstm analysis or networks techniques to overcome neurons\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "lstm analysis or networks techniques to overcome neurons gated\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "lstm analysis or networks techniques to overcome neurons gated the\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "lstm analysis or networks techniques to overcome neurons gated the units\n"
          ]
        }
      ]
    }
  ]
}